/*
 * This file was automatically generated by EvoSuite
 * Sun Nov 10 23:49:49 GMT 2024
 */

package org.apache.hadoop.hdfs.protocol.proto;

import org.junit.Test;
import static org.junit.Assert.*;
import static org.evosuite.shaded.org.mockito.Mockito.*;
import static org.evosuite.runtime.EvoAssertions.*;
import java.io.ByteArrayInputStream;
import java.io.DataInputStream;
import java.io.FileDescriptor;
import java.io.IOException;
import java.io.InputStream;
import java.io.PipedInputStream;
import java.io.PushbackInputStream;
import java.io.SequenceInputStream;
import java.nio.ByteBuffer;
import java.util.Enumeration;
import java.util.Map;
import java.util.PriorityQueue;
import java.util.SortedSet;
import java.util.TreeSet;
import java.util.concurrent.LinkedBlockingDeque;
import java.util.concurrent.LinkedBlockingQueue;
import java.util.concurrent.LinkedTransferQueue;
import java.util.concurrent.SynchronousQueue;
import org.apache.hadoop.hdfs.protocol.proto.HdfsProtos;
import org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos;
import org.apache.hadoop.thirdparty.protobuf.ByteString;
import org.apache.hadoop.thirdparty.protobuf.CodedInputStream;
import org.apache.hadoop.thirdparty.protobuf.DescriptorProtos;
import org.apache.hadoop.thirdparty.protobuf.Descriptors;
import org.apache.hadoop.thirdparty.protobuf.DynamicMessage;
import org.apache.hadoop.thirdparty.protobuf.ExtensionRegistry;
import org.apache.hadoop.thirdparty.protobuf.ExtensionRegistryLite;
import org.apache.hadoop.thirdparty.protobuf.Field;
import org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3;
import org.apache.hadoop.thirdparty.protobuf.Internal;
import org.apache.hadoop.thirdparty.protobuf.Message;
import org.apache.hadoop.thirdparty.protobuf.Parser;
import org.apache.hadoop.thirdparty.protobuf.Timestamp;
import org.apache.hadoop.thirdparty.protobuf.UnknownFieldSet;
import org.eclipse.jetty.io.LogarithmicArrayByteBufferPool;
import org.eclipse.jetty.io.MappedByteBufferPool;
import org.eclipse.jetty.io.NullByteBufferPool;
import org.evosuite.runtime.EvoRunner;
import org.evosuite.runtime.EvoRunnerParameters;
import org.evosuite.runtime.ViolatedAssumptionAnswer;
import org.evosuite.runtime.mock.java.io.MockFileInputStream;
import org.junit.runner.RunWith;

@RunWith(EvoRunner.class) @EvoRunnerParameters(mockJVMNonDeterminism = true, useVFS = true, useVNET = true, resetStaticState = true, separateClassLoader = true) 
public class HdfsServerProtos_ESTest extends HdfsServerProtos_ESTest_scaffolding {

  @Test(timeout = 4000)
  public void test000()  throws Throwable  {
      byte[] byteArray0 = new byte[4];
      ByteArrayInputStream byteArrayInputStream0 = new ByteArrayInputStream(byteArray0, (byte)0, 0);
      HdfsServerProtos.BlockWithLocationsProto hdfsServerProtos_BlockWithLocationsProto0 = HdfsServerProtos.BlockWithLocationsProto.parseDelimitedFrom((InputStream) byteArrayInputStream0);
      assertNull(hdfsServerProtos_BlockWithLocationsProto0);
  }

  @Test(timeout = 4000)
  public void test001()  throws Throwable  {
      HdfsServerProtos.CheckpointSignatureProto hdfsServerProtos_CheckpointSignatureProto0 = HdfsServerProtos.CheckpointSignatureProto.getDefaultInstance();
      HdfsServerProtos.CheckpointSignatureProto.Builder hdfsServerProtos_CheckpointSignatureProto_Builder0 = hdfsServerProtos_CheckpointSignatureProto0.newBuilderForType();
      assertEquals(0L, hdfsServerProtos_CheckpointSignatureProto_Builder0.getMostRecentCheckpointTxId());
  }

  @Test(timeout = 4000)
  public void test002()  throws Throwable  {
      HdfsServerProtos.CheckpointCommandProto.Builder hdfsServerProtos_CheckpointCommandProto_Builder0 = HdfsServerProtos.CheckpointCommandProto.newBuilder();
      assertFalse(hdfsServerProtos_CheckpointCommandProto_Builder0.hasSignature());
  }

  @Test(timeout = 4000)
  public void test003()  throws Throwable  {
      HdfsServerProtos.CheckpointCommandProto hdfsServerProtos_CheckpointCommandProto0 = HdfsServerProtos.CheckpointCommandProto.getDefaultInstance();
      assertEquals(2, HdfsServerProtos.CheckpointCommandProto.NEEDTORETURNIMAGE_FIELD_NUMBER);
  }

  @Test(timeout = 4000)
  public void test004()  throws Throwable  {
      HdfsServerProtos.BlockWithLocationsProto hdfsServerProtos_BlockWithLocationsProto0 = HdfsServerProtos.BlockWithLocationsProto.getDefaultInstance();
      HdfsServerProtos.BlockWithLocationsProto.Builder hdfsServerProtos_BlockWithLocationsProto_Builder0 = hdfsServerProtos_BlockWithLocationsProto0.toBuilder();
      assertFalse(hdfsServerProtos_BlockWithLocationsProto_Builder0.hasIndices());
  }

  @Test(timeout = 4000)
  public void test005()  throws Throwable  {
      FileDescriptor fileDescriptor0 = new FileDescriptor();
      MockFileInputStream mockFileInputStream0 = new MockFileInputStream(fileDescriptor0);
      try { 
        HdfsServerProtos.RecoveringBlockProto.parseFrom((InputStream) mockFileInputStream0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.evosuite.runtime.mock.java.io.NativeMockedIO", e);
      }
  }

  @Test(timeout = 4000)
  public void test006()  throws Throwable  {
      byte[] byteArray0 = new byte[6];
      byteArray0[0] = (byte) (-92);
      ByteArrayInputStream byteArrayInputStream0 = new ByteArrayInputStream(byteArray0);
      ExtensionRegistry extensionRegistry0 = ExtensionRegistry.getEmptyRegistry();
      try { 
        HdfsServerProtos.NamespaceInfoProto.parseDelimitedFrom((InputStream) byteArrayInputStream0, (ExtensionRegistryLite) extensionRegistry0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Protocol message contained an invalid tag (zero).
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException", e);
      }
  }

  @Test(timeout = 4000)
  public void test007()  throws Throwable  {
      FileDescriptor fileDescriptor0 = new FileDescriptor();
      MockFileInputStream mockFileInputStream0 = new MockFileInputStream(fileDescriptor0);
      ExtensionRegistryLite extensionRegistryLite0 = ExtensionRegistryLite.getEmptyRegistry();
      try { 
        HdfsServerProtos.BlocksWithLocationsProto.parseFrom((InputStream) mockFileInputStream0, extensionRegistryLite0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.evosuite.runtime.mock.java.io.NativeMockedIO", e);
      }
  }

  @Test(timeout = 4000)
  public void test008()  throws Throwable  {
      byte[] byteArray0 = new byte[6];
      byteArray0[0] = (byte)16;
      ExtensionRegistryLite extensionRegistryLite0 = ExtensionRegistryLite.getEmptyRegistry();
      try { 
        HdfsServerProtos.VersionResponseProto.parseFrom(byteArray0, extensionRegistryLite0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Protocol message contained an invalid tag (zero).
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException", e);
      }
  }

  @Test(timeout = 4000)
  public void test009()  throws Throwable  {
      HdfsServerProtos.RemoteEditLogManifestProto.Builder hdfsServerProtos_RemoteEditLogManifestProto_Builder0 = HdfsServerProtos.RemoteEditLogManifestProto.newBuilder();
      HdfsServerProtos.RemoteEditLogManifestProto.Builder hdfsServerProtos_RemoteEditLogManifestProto_Builder1 = hdfsServerProtos_RemoteEditLogManifestProto_Builder0.clear();
      assertEquals(0L, hdfsServerProtos_RemoteEditLogManifestProto_Builder1.getCommittedTxnId());
      assertFalse(hdfsServerProtos_RemoteEditLogManifestProto_Builder1.hasCommittedTxnId());
  }

  @Test(timeout = 4000)
  public void test010()  throws Throwable  {
      LinkedTransferQueue<ByteBuffer> linkedTransferQueue0 = new LinkedTransferQueue<ByteBuffer>();
      LinkedBlockingQueue<ByteBuffer> linkedBlockingQueue0 = new LinkedBlockingQueue<ByteBuffer>(linkedTransferQueue0);
      CodedInputStream codedInputStream0 = CodedInputStream.newInstance((Iterable<ByteBuffer>) linkedBlockingQueue0);
      ExtensionRegistryLite extensionRegistryLite0 = ExtensionRegistryLite.newInstance();
      try { 
        HdfsServerProtos.StorageInfoProto.parseFrom(codedInputStream0, extensionRegistryLite0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Message missing required fields: layoutVersion, namespceID, clusterID, cTime
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.UninitializedMessageException", e);
      }
  }

  @Test(timeout = 4000)
  public void test011()  throws Throwable  {
      SynchronousQueue<ByteBuffer> synchronousQueue0 = new SynchronousQueue<ByteBuffer>(true);
      LinkedBlockingDeque<ByteBuffer> linkedBlockingDeque0 = new LinkedBlockingDeque<ByteBuffer>(synchronousQueue0);
      CodedInputStream codedInputStream0 = CodedInputStream.newInstance((Iterable<ByteBuffer>) linkedBlockingDeque0);
      ExtensionRegistryLite extensionRegistryLite0 = ExtensionRegistryLite.getEmptyRegistry();
      try { 
        HdfsServerProtos.BlockKeyProto.parseFrom(codedInputStream0, extensionRegistryLite0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Message missing required fields: keyId, expiryDate
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.UninitializedMessageException", e);
      }
  }

  @Test(timeout = 4000)
  public void test012()  throws Throwable  {
      HdfsServerProtos.RemoteEditLogManifestProto.Builder hdfsServerProtos_RemoteEditLogManifestProto_Builder0 = HdfsServerProtos.RemoteEditLogManifestProto.newBuilder();
      // Undeclared exception!
      try { 
        hdfsServerProtos_RemoteEditLogManifestProto_Builder0.getLogsBuilder(34);
        fail("Expecting exception: IndexOutOfBoundsException");
      
      } catch(IndexOutOfBoundsException e) {
         //
         // Index: 34, Size: 0
         //
         verifyException("java.util.ArrayList", e);
      }
  }

  @Test(timeout = 4000)
  public void test013()  throws Throwable  {
      ExtensionRegistry extensionRegistry0 = ExtensionRegistry.newInstance();
      HdfsServerProtos.BlocksWithLocationsProto hdfsServerProtos_BlocksWithLocationsProto0 = HdfsServerProtos.BlocksWithLocationsProto.parseFrom((InputStream) null, (ExtensionRegistryLite) extensionRegistry0);
      HdfsServerProtos.BlocksWithLocationsProto.Builder hdfsServerProtos_BlocksWithLocationsProto_Builder0 = HdfsServerProtos.BlocksWithLocationsProto.newBuilder(hdfsServerProtos_BlocksWithLocationsProto0);
      assertEquals(0, hdfsServerProtos_BlocksWithLocationsProto_Builder0.getBlocksCount());
  }

  @Test(timeout = 4000)
  public void test014()  throws Throwable  {
      byte[] byteArray0 = new byte[6];
      byteArray0[0] = (byte)16;
      try { 
        HdfsServerProtos.NNHAStatusHeartbeatProto.parseFrom(byteArray0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Protocol message contained an invalid tag (zero).
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException", e);
      }
  }

  @Test(timeout = 4000)
  public void test015()  throws Throwable  {
      HdfsServerProtos.NamenodeRegistrationProto.Builder hdfsServerProtos_NamenodeRegistrationProto_Builder0 = HdfsServerProtos.NamenodeRegistrationProto.newBuilder();
      HdfsServerProtos.NamenodeRegistrationProto.Builder hdfsServerProtos_NamenodeRegistrationProto_Builder1 = hdfsServerProtos_NamenodeRegistrationProto_Builder0.clearStorageInfo();
      assertFalse(hdfsServerProtos_NamenodeRegistrationProto_Builder1.hasRpcAddress());
  }

  @Test(timeout = 4000)
  public void test016()  throws Throwable  {
      ByteString byteString0 = ByteString.copyFromUtf8("vx@$4~");
      InputStream inputStream0 = byteString0.newInput();
      DataInputStream dataInputStream0 = new DataInputStream(inputStream0);
      dataInputStream0.readUnsignedShort();
      dataInputStream0.readInt();
      try { 
        HdfsServerProtos.NamenodeRegistrationProto.parseFrom((InputStream) dataInputStream0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Message missing required fields: rpcAddress, httpAddress, storageInfo
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.UninitializedMessageException", e);
      }
  }

  @Test(timeout = 4000)
  public void test017()  throws Throwable  {
      ByteString byteString0 = ByteString.copyFromUtf8("vx@$4~");
      InputStream inputStream0 = byteString0.newInput();
      DataInputStream dataInputStream0 = new DataInputStream(inputStream0);
      dataInputStream0.readUnsignedShort();
      try { 
        HdfsServerProtos.NamenodeRegistrationProto.parseFrom((InputStream) dataInputStream0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Protocol message end-group tag did not match expected tag.
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException", e);
      }
  }

  @Test(timeout = 4000)
  public void test018()  throws Throwable  {
      HdfsServerProtos.NamenodeRegistrationProto.Builder hdfsServerProtos_NamenodeRegistrationProto_Builder0 = HdfsServerProtos.NamenodeRegistrationProto.newBuilder();
      HdfsServerProtos.NamenodeRegistrationProto.Builder hdfsServerProtos_NamenodeRegistrationProto_Builder1 = hdfsServerProtos_NamenodeRegistrationProto_Builder0.clear();
      assertFalse(hdfsServerProtos_NamenodeRegistrationProto_Builder1.hasRpcAddress());
  }

  @Test(timeout = 4000)
  public void test019()  throws Throwable  {
      HdfsServerProtos.NamenodeRegistrationProto.NamenodeRoleProto.forNumber((-17));
  }

  @Test(timeout = 4000)
  public void test020()  throws Throwable  {
      byte[] byteArray0 = new byte[6];
      byteArray0[0] = (byte) (-41);
      try { 
        HdfsServerProtos.StorageInfoProto.parseFrom(byteArray0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Protocol message tag had invalid wire type.
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException", e);
      }
  }

  @Test(timeout = 4000)
  public void test021()  throws Throwable  {
      // Undeclared exception!
      try { 
        HdfsServerProtos.VersionResponseProto.newBuilder((HdfsServerProtos.VersionResponseProto) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionResponseProto$Builder", e);
      }
  }

  @Test(timeout = 4000)
  public void test022()  throws Throwable  {
      HdfsServerProtos.NamenodeCommandProto.Builder hdfsServerProtos_NamenodeCommandProto_Builder0 = HdfsServerProtos.NamenodeCommandProto.newBuilder();
      Map<Descriptors.FieldDescriptor, Object> map0 = hdfsServerProtos_NamenodeCommandProto_Builder0.getAllFields();
      assertEquals(HdfsServerProtos.NamenodeCommandProto.Type.NamenodeCommand, hdfsServerProtos_NamenodeCommandProto_Builder0.getType());
      assertTrue(map0.isEmpty());
  }

  @Test(timeout = 4000)
  public void test023()  throws Throwable  {
      HdfsServerProtos.CheckpointSignatureProto.Builder hdfsServerProtos_CheckpointSignatureProto_Builder0 = HdfsServerProtos.CheckpointSignatureProto.newBuilder();
      HdfsServerProtos.CheckpointSignatureProto.Builder hdfsServerProtos_CheckpointSignatureProto_Builder1 = hdfsServerProtos_CheckpointSignatureProto_Builder0.clearStorageInfo();
      assertFalse(hdfsServerProtos_CheckpointSignatureProto_Builder1.hasBlockPoolId());
  }

  @Test(timeout = 4000)
  public void test024()  throws Throwable  {
      HdfsServerProtos.CheckpointSignatureProto.Builder hdfsServerProtos_CheckpointSignatureProto_Builder0 = HdfsServerProtos.CheckpointSignatureProto.newBuilder();
      HdfsServerProtos.StorageInfoProto.Builder hdfsServerProtos_StorageInfoProto_Builder0 = HdfsServerProtos.StorageInfoProto.newBuilder();
      // Undeclared exception!
      try { 
        hdfsServerProtos_CheckpointSignatureProto_Builder0.setStorageInfo(hdfsServerProtos_StorageInfoProto_Builder0);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // Message missing required fields: layoutVersion, namespceID, clusterID, cTime
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.AbstractMessage$Builder", e);
      }
  }

  @Test(timeout = 4000)
  public void test025()  throws Throwable  {
      byte[] byteArray0 = new byte[1];
      byteArray0[0] = (byte)101;
      try { 
        HdfsServerProtos.CheckpointSignatureProto.parseFrom(byteArray0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // While parsing a protocol message, the input ended unexpectedly in the middle of a field.  This could mean either that the input has been truncated or that an embedded message misreported its own length.
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException", e);
      }
  }

  @Test(timeout = 4000)
  public void test026()  throws Throwable  {
      HdfsServerProtos.RecoveringBlockProto.Builder hdfsServerProtos_RecoveringBlockProto_Builder0 = HdfsServerProtos.RecoveringBlockProto.newBuilder();
      HdfsServerProtos.RecoveringBlockProto.Builder hdfsServerProtos_RecoveringBlockProto_Builder1 = hdfsServerProtos_RecoveringBlockProto_Builder0.clearEcPolicy();
      assertFalse(hdfsServerProtos_RecoveringBlockProto_Builder1.hasNewGenStamp());
  }

  @Test(timeout = 4000)
  public void test027()  throws Throwable  {
      HdfsServerProtos.RecoveringBlockProto.Builder hdfsServerProtos_RecoveringBlockProto_Builder0 = HdfsServerProtos.RecoveringBlockProto.newBuilder();
      HdfsProtos.BlockProto.Builder hdfsProtos_BlockProto_Builder0 = HdfsProtos.BlockProto.newBuilder();
      // Undeclared exception!
      try { 
        hdfsServerProtos_RecoveringBlockProto_Builder0.setTruncateBlock(hdfsProtos_BlockProto_Builder0);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // Message missing required fields: blockId, genStamp
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.AbstractMessage$Builder", e);
      }
  }

  @Test(timeout = 4000)
  public void test028()  throws Throwable  {
      byte[] byteArray0 = new byte[2];
      byteArray0[0] = (byte)28;
      try { 
        HdfsServerProtos.RecoveringBlockProto.parseFrom(byteArray0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Protocol message end-group tag did not match expected tag.
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException", e);
      }
  }

  @Test(timeout = 4000)
  public void test029()  throws Throwable  {
      HdfsServerProtos.RecoveringBlockProto.Builder hdfsServerProtos_RecoveringBlockProto_Builder0 = HdfsServerProtos.RecoveringBlockProto.newBuilder();
      byte[] byteArray0 = new byte[0];
      Field field0 = Field.parseFrom(byteArray0);
      ByteString byteString0 = field0.getTypeUrlBytes();
      // Undeclared exception!
      try { 
        hdfsServerProtos_RecoveringBlockProto_Builder0.mergeFrom(byteString0, (ExtensionRegistryLite) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RecoveringBlockProto$Builder", e);
      }
  }

  @Test(timeout = 4000)
  public void test030()  throws Throwable  {
      HdfsServerProtos.RecoveringBlockProto.Builder hdfsServerProtos_RecoveringBlockProto_Builder0 = HdfsServerProtos.RecoveringBlockProto.newBuilder();
      HdfsServerProtos.RecoveringBlockProto.Builder hdfsServerProtos_RecoveringBlockProto_Builder1 = hdfsServerProtos_RecoveringBlockProto_Builder0.clear();
      assertFalse(hdfsServerProtos_RecoveringBlockProto_Builder1.hasNewGenStamp());
      assertEquals(0L, hdfsServerProtos_RecoveringBlockProto_Builder1.getNewGenStamp());
  }

  @Test(timeout = 4000)
  public void test031()  throws Throwable  {
      HdfsServerProtos.RemoteEditLogManifestProto.Builder hdfsServerProtos_RemoteEditLogManifestProto_Builder0 = HdfsServerProtos.RemoteEditLogManifestProto.newBuilder();
      byte[] byteArray0 = new byte[7];
      byteArray0[0] = (byte)62;
      CodedInputStream codedInputStream0 = CodedInputStream.newInstance(byteArray0);
      ExtensionRegistryLite extensionRegistryLite0 = ExtensionRegistryLite.getEmptyRegistry();
      try { 
        hdfsServerProtos_RemoteEditLogManifestProto_Builder0.mergeFrom(codedInputStream0, extensionRegistryLite0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Protocol message tag had invalid wire type.
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException", e);
      }
  }

  @Test(timeout = 4000)
  public void test032()  throws Throwable  {
      byte[] byteArray0 = new byte[6];
      byteArray0[0] = (byte) (-114);
      ExtensionRegistryLite extensionRegistryLite0 = ExtensionRegistryLite.getEmptyRegistry();
      try { 
        HdfsServerProtos.RemoteEditLogProto.parseFrom(byteArray0, extensionRegistryLite0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Protocol message tag had invalid wire type.
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException", e);
      }
  }

  @Test(timeout = 4000)
  public void test033()  throws Throwable  {
      HdfsServerProtos.BlocksWithLocationsProto hdfsServerProtos_BlocksWithLocationsProto0 = HdfsServerProtos.BlocksWithLocationsProto.getDefaultInstance();
      HdfsServerProtos.BlocksWithLocationsProto.Builder hdfsServerProtos_BlocksWithLocationsProto_Builder0 = hdfsServerProtos_BlocksWithLocationsProto0.newBuilderForType();
      // Undeclared exception!
      try { 
        hdfsServerProtos_BlocksWithLocationsProto_Builder0.removeBlocks(1);
        fail("Expecting exception: IndexOutOfBoundsException");
      
      } catch(IndexOutOfBoundsException e) {
         //
         // Index: 1, Size: 0
         //
         verifyException("java.util.ArrayList", e);
      }
  }

  @Test(timeout = 4000)
  public void test034()  throws Throwable  {
      HdfsServerProtos.BlocksWithLocationsProto hdfsServerProtos_BlocksWithLocationsProto0 = HdfsServerProtos.BlocksWithLocationsProto.getDefaultInstance();
      HdfsServerProtos.BlocksWithLocationsProto.Builder hdfsServerProtos_BlocksWithLocationsProto_Builder0 = hdfsServerProtos_BlocksWithLocationsProto0.newBuilderForType();
      hdfsServerProtos_BlocksWithLocationsProto_Builder0.addBlocksBuilder();
      // Undeclared exception!
      try { 
        hdfsServerProtos_BlocksWithLocationsProto_Builder0.removeBlocks(1);
        fail("Expecting exception: IndexOutOfBoundsException");
      
      } catch(IndexOutOfBoundsException e) {
         //
         // Index: 1, Size: 1
         //
         verifyException("java.util.ArrayList", e);
      }
  }

  @Test(timeout = 4000)
  public void test035()  throws Throwable  {
      HdfsServerProtos.BlocksWithLocationsProto hdfsServerProtos_BlocksWithLocationsProto0 = HdfsServerProtos.BlocksWithLocationsProto.getDefaultInstance();
      HdfsServerProtos.BlocksWithLocationsProto.Builder hdfsServerProtos_BlocksWithLocationsProto_Builder0 = hdfsServerProtos_BlocksWithLocationsProto0.newBuilderForType();
      HdfsServerProtos.BlocksWithLocationsProto.Builder hdfsServerProtos_BlocksWithLocationsProto_Builder1 = hdfsServerProtos_BlocksWithLocationsProto_Builder0.clearBlocks();
      assertEquals(0, hdfsServerProtos_BlocksWithLocationsProto_Builder1.getBlocksCount());
  }

  @Test(timeout = 4000)
  public void test036()  throws Throwable  {
      HdfsServerProtos.BlocksWithLocationsProto.Builder hdfsServerProtos_BlocksWithLocationsProto_Builder0 = HdfsServerProtos.BlocksWithLocationsProto.newBuilder();
      // Undeclared exception!
      try { 
        hdfsServerProtos_BlocksWithLocationsProto_Builder0.addBlocks(876, (HdfsServerProtos.BlockWithLocationsProto) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlocksWithLocationsProto$Builder", e);
      }
  }

  @Test(timeout = 4000)
  public void test037()  throws Throwable  {
      byte[] byteArray0 = new byte[5];
      byteArray0[0] = (byte)42;
      ExtensionRegistry extensionRegistry0 = ExtensionRegistry.getEmptyRegistry();
      try { 
        HdfsServerProtos.BlocksWithLocationsProto.parseFrom(byteArray0, (ExtensionRegistryLite) extensionRegistry0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Protocol message contained an invalid tag (zero).
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException", e);
      }
  }

  @Test(timeout = 4000)
  public void test038()  throws Throwable  {
      byte[] byteArray0 = new byte[7];
      // Undeclared exception!
      try { 
        HdfsServerProtos.BlocksWithLocationsProto.parseFrom(byteArray0, (ExtensionRegistryLite) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlocksWithLocationsProto$Builder", e);
      }
  }

  @Test(timeout = 4000)
  public void test039()  throws Throwable  {
      HdfsServerProtos.BlocksWithLocationsProto hdfsServerProtos_BlocksWithLocationsProto0 = HdfsServerProtos.BlocksWithLocationsProto.getDefaultInstance();
      HdfsServerProtos.BlocksWithLocationsProto.Builder hdfsServerProtos_BlocksWithLocationsProto_Builder0 = hdfsServerProtos_BlocksWithLocationsProto0.newBuilderForType();
      HdfsServerProtos.BlocksWithLocationsProto.Builder hdfsServerProtos_BlocksWithLocationsProto_Builder1 = hdfsServerProtos_BlocksWithLocationsProto_Builder0.mergeFrom((Message) hdfsServerProtos_BlocksWithLocationsProto0);
      assertEquals(0, hdfsServerProtos_BlocksWithLocationsProto_Builder1.getBlocksCount());
  }

  @Test(timeout = 4000)
  public void test040()  throws Throwable  {
      HdfsServerProtos.BlocksWithLocationsProto hdfsServerProtos_BlocksWithLocationsProto0 = HdfsServerProtos.BlocksWithLocationsProto.getDefaultInstance();
      HdfsServerProtos.BlocksWithLocationsProto.Builder hdfsServerProtos_BlocksWithLocationsProto_Builder0 = hdfsServerProtos_BlocksWithLocationsProto0.newBuilderForType();
      HdfsServerProtos.BlocksWithLocationsProto.Builder hdfsServerProtos_BlocksWithLocationsProto_Builder1 = hdfsServerProtos_BlocksWithLocationsProto_Builder0.clear();
      assertSame(hdfsServerProtos_BlocksWithLocationsProto_Builder1, hdfsServerProtos_BlocksWithLocationsProto_Builder0);
  }

  @Test(timeout = 4000)
  public void test041()  throws Throwable  {
      ExtensionRegistry extensionRegistry0 = ExtensionRegistry.newInstance();
      HdfsServerProtos.BlocksWithLocationsProto hdfsServerProtos_BlocksWithLocationsProto0 = HdfsServerProtos.BlocksWithLocationsProto.parseFrom((InputStream) null, (ExtensionRegistryLite) extensionRegistry0);
      boolean boolean0 = hdfsServerProtos_BlocksWithLocationsProto0.equals(extensionRegistry0);
      assertFalse(boolean0);
  }

  @Test(timeout = 4000)
  public void test042()  throws Throwable  {
      HdfsServerProtos.BlockWithLocationsProto hdfsServerProtos_BlockWithLocationsProto0 = HdfsServerProtos.BlockWithLocationsProto.getDefaultInstance();
      HdfsServerProtos.BlockWithLocationsProto.Builder hdfsServerProtos_BlockWithLocationsProto_Builder0 = HdfsServerProtos.BlockWithLocationsProto.newBuilder(hdfsServerProtos_BlockWithLocationsProto0);
      HdfsProtos.StorageTypeProto hdfsProtos_StorageTypeProto0 = HdfsProtos.StorageTypeProto.NVDIMM;
      // Undeclared exception!
      try { 
        hdfsServerProtos_BlockWithLocationsProto_Builder0.setStorageTypes((-1519), hdfsProtos_StorageTypeProto0);
        fail("Expecting exception: ArrayIndexOutOfBoundsException");
      
      } catch(ArrayIndexOutOfBoundsException e) {
         //
         // no message in exception (getMessage() returned null)
         //
      }
  }

  @Test(timeout = 4000)
  public void test043()  throws Throwable  {
      HdfsServerProtos.BlockWithLocationsProto.Builder hdfsServerProtos_BlockWithLocationsProto_Builder0 = HdfsServerProtos.BlockWithLocationsProto.newBuilder();
      // Undeclared exception!
      try { 
        hdfsServerProtos_BlockWithLocationsProto_Builder0.setDatanodeUuids(32, "FXP)izXnR|+>P");
        fail("Expecting exception: IndexOutOfBoundsException");
      
      } catch(IndexOutOfBoundsException e) {
         //
         // Index: 32, Size: 0
         //
         verifyException("java.util.ArrayList", e);
      }
  }

  @Test(timeout = 4000)
  public void test044()  throws Throwable  {
      HdfsServerProtos.BlockWithLocationsProto.Builder hdfsServerProtos_BlockWithLocationsProto_Builder0 = HdfsServerProtos.BlockWithLocationsProto.newBuilder();
      HdfsProtos.BlockProto hdfsProtos_BlockProto0 = HdfsProtos.BlockProto.getDefaultInstance();
      hdfsServerProtos_BlockWithLocationsProto_Builder0.setBlock(hdfsProtos_BlockProto0);
      assertTrue(hdfsServerProtos_BlockWithLocationsProto_Builder0.hasBlock());
  }

  @Test(timeout = 4000)
  public void test045()  throws Throwable  {
      byte[] byteArray0 = new byte[4];
      byteArray0[0] = (byte)9;
      try { 
        HdfsServerProtos.BlockWithLocationsProto.parseFrom(byteArray0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // While parsing a protocol message, the input ended unexpectedly in the middle of a field.  This could mean either that the input has been truncated or that an embedded message misreported its own length.
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException", e);
      }
  }

  @Test(timeout = 4000)
  public void test046()  throws Throwable  {
      byte[] byteArray0 = new byte[6];
      byteArray0[0] = (byte)42;
      ByteString byteString0 = ByteString.copyFrom(byteArray0);
      try { 
        HdfsServerProtos.BlockWithLocationsProto.parseFrom(byteString0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Protocol message contained an invalid tag (zero).
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException", e);
      }
  }

  @Test(timeout = 4000)
  public void test047()  throws Throwable  {
      byte[] byteArray0 = new byte[4];
      byteArray0[0] = (byte) (-27);
      ExtensionRegistry extensionRegistry0 = ExtensionRegistry.newInstance();
      try { 
        HdfsServerProtos.ExportedBlockKeysProto.parseFrom(byteArray0, (ExtensionRegistryLite) extensionRegistry0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // While parsing a protocol message, the input ended unexpectedly in the middle of a field.  This could mean either that the input has been truncated or that an embedded message misreported its own length.
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException", e);
      }
  }

  @Test(timeout = 4000)
  public void test048()  throws Throwable  {
      HdfsServerProtos.ExportedBlockKeysProto.Builder hdfsServerProtos_ExportedBlockKeysProto_Builder0 = HdfsServerProtos.ExportedBlockKeysProto.newBuilder();
      // Undeclared exception!
      try { 
        hdfsServerProtos_ExportedBlockKeysProto_Builder0.mergeFrom((Message) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.AbstractMessage$Builder", e);
      }
  }

  @Test(timeout = 4000)
  public void test049()  throws Throwable  {
      ExtensionRegistryLite extensionRegistryLite0 = ExtensionRegistryLite.getEmptyRegistry();
      byte[] byteArray0 = new byte[4];
      byteArray0[0] = (byte)84;
      try { 
        HdfsServerProtos.BlockKeyProto.parseFrom(byteArray0, extensionRegistryLite0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Protocol message end-group tag did not match expected tag.
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException", e);
      }
  }

  @Test(timeout = 4000)
  public void test050()  throws Throwable  {
      MappedByteBufferPool.Tagged mappedByteBufferPool_Tagged0 = new MappedByteBufferPool.Tagged();
      ByteBuffer byteBuffer0 = mappedByteBufferPool_Tagged0.newByteBuffer(53, true);
      try { 
        HdfsServerProtos.RemoteEditLogProto.parseFrom(byteBuffer0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Message missing required fields: startTxId, endTxId
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.UninitializedMessageException", e);
      }
  }

  @Test(timeout = 4000)
  public void test051()  throws Throwable  {
      // Undeclared exception!
      try { 
        HdfsServerProtos.RemoteEditLogProto.newBuilder((HdfsServerProtos.RemoteEditLogProto) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$RemoteEditLogProto$Builder", e);
      }
  }

  @Test(timeout = 4000)
  public void test052()  throws Throwable  {
      byte[] byteArray0 = new byte[2];
      ByteArrayInputStream byteArrayInputStream0 = new ByteArrayInputStream(byteArray0);
      try { 
        HdfsServerProtos.RemoteEditLogProto.parseDelimitedFrom((InputStream) byteArrayInputStream0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Message missing required fields: startTxId, endTxId
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.UninitializedMessageException", e);
      }
  }

  @Test(timeout = 4000)
  public void test053()  throws Throwable  {
      MappedByteBufferPool.Tagged mappedByteBufferPool_Tagged0 = new MappedByteBufferPool.Tagged();
      ByteBuffer byteBuffer0 = mappedByteBufferPool_Tagged0.newByteBuffer(24, false);
      ExtensionRegistry extensionRegistry0 = ExtensionRegistry.getEmptyRegistry();
      try { 
        HdfsServerProtos.RemoteEditLogProto.parseFrom(byteBuffer0, (ExtensionRegistryLite) extensionRegistry0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Message missing required fields: startTxId, endTxId
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.UninitializedMessageException", e);
      }
  }

  @Test(timeout = 4000)
  public void test054()  throws Throwable  {
      HdfsServerProtos.NamenodeRegistrationProto.Builder hdfsServerProtos_NamenodeRegistrationProto_Builder0 = HdfsServerProtos.NamenodeRegistrationProto.newBuilder();
      HdfsServerProtos.NamenodeRegistrationProto.NamenodeRoleProto hdfsServerProtos_NamenodeRegistrationProto_NamenodeRoleProto0 = HdfsServerProtos.NamenodeRegistrationProto.NamenodeRoleProto.CHECKPOINT;
      hdfsServerProtos_NamenodeRegistrationProto_Builder0.setRole(hdfsServerProtos_NamenodeRegistrationProto_NamenodeRoleProto0);
      assertTrue(hdfsServerProtos_NamenodeRegistrationProto_Builder0.hasRole());
  }

  @Test(timeout = 4000)
  public void test055()  throws Throwable  {
      // Undeclared exception!
      try { 
        HdfsServerProtos.CheckpointSignatureProto.parseFrom((ByteString) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.AbstractParser", e);
      }
  }

  @Test(timeout = 4000)
  public void test056()  throws Throwable  {
      PipedInputStream pipedInputStream0 = new PipedInputStream();
      CodedInputStream codedInputStream0 = CodedInputStream.newInstance((InputStream) pipedInputStream0);
      ExtensionRegistryLite extensionRegistryLite0 = ExtensionRegistryLite.newInstance();
      try { 
        HdfsServerProtos.CheckpointSignatureProto.parseFrom(codedInputStream0, extensionRegistryLite0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Pipe not connected
         //
         verifyException("java.io.PipedInputStream", e);
      }
  }

  @Test(timeout = 4000)
  public void test057()  throws Throwable  {
      Parser<HdfsServerProtos.CheckpointSignatureProto> parser0 = HdfsServerProtos.CheckpointSignatureProto.parser();
      assertNotNull(parser0);
  }

  @Test(timeout = 4000)
  public void test058()  throws Throwable  {
      byte[] byteArray0 = new byte[9];
      ByteBuffer byteBuffer0 = ByteBuffer.wrap(byteArray0);
      // Undeclared exception!
      try { 
        HdfsServerProtos.CheckpointSignatureProto.parseFrom(byteBuffer0, (ExtensionRegistryLite) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointSignatureProto$Builder", e);
      }
  }

  @Test(timeout = 4000)
  public void test059()  throws Throwable  {
      ByteBuffer byteBuffer0 = ByteBuffer.allocate(26);
      try { 
        HdfsServerProtos.CheckpointSignatureProto.parseFrom(byteBuffer0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Protocol message contained an invalid tag (zero).
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException", e);
      }
  }

  @Test(timeout = 4000)
  public void test060()  throws Throwable  {
      ByteBuffer byteBuffer0 = ByteBuffer.allocate(24);
      try { 
        HdfsServerProtos.ExportedBlockKeysProto.parseFrom(byteBuffer0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Protocol message contained an invalid tag (zero).
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException", e);
      }
  }

  @Test(timeout = 4000)
  public void test061()  throws Throwable  {
      PipedInputStream pipedInputStream0 = new PipedInputStream();
      try { 
        HdfsServerProtos.ExportedBlockKeysProto.parseDelimitedFrom((InputStream) pipedInputStream0, (ExtensionRegistryLite) null);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Pipe not connected
         //
         verifyException("java.io.PipedInputStream", e);
      }
  }

  @Test(timeout = 4000)
  public void test062()  throws Throwable  {
      LogarithmicArrayByteBufferPool logarithmicArrayByteBufferPool0 = new LogarithmicArrayByteBufferPool((-1639), (-1639));
      ByteBuffer byteBuffer0 = logarithmicArrayByteBufferPool0.newByteBuffer(4384, true);
      // Undeclared exception!
      try { 
        HdfsServerProtos.ExportedBlockKeysProto.parseFrom(byteBuffer0, (ExtensionRegistryLite) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$ExportedBlockKeysProto$Builder", e);
      }
  }

  @Test(timeout = 4000)
  public void test063()  throws Throwable  {
      // Undeclared exception!
      try { 
        HdfsServerProtos.ExportedBlockKeysProto.parseDelimitedFrom((InputStream) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.AbstractParser", e);
      }
  }

  @Test(timeout = 4000)
  public void test064()  throws Throwable  {
      HdfsServerProtos.ExportedBlockKeysProto hdfsServerProtos_ExportedBlockKeysProto0 = HdfsServerProtos.ExportedBlockKeysProto.getDefaultInstance();
      HdfsServerProtos.ExportedBlockKeysProto.Builder hdfsServerProtos_ExportedBlockKeysProto_Builder0 = hdfsServerProtos_ExportedBlockKeysProto0.newBuilderForType();
      assertFalse(hdfsServerProtos_ExportedBlockKeysProto_Builder0.hasTokenLifeTime());
  }

  @Test(timeout = 4000)
  public void test065()  throws Throwable  {
      // Undeclared exception!
      try { 
        HdfsServerProtos.ExportedBlockKeysProto.parseFrom((ByteString) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.AbstractParser", e);
      }
  }

  @Test(timeout = 4000)
  public void test066()  throws Throwable  {
      NullByteBufferPool nullByteBufferPool0 = new NullByteBufferPool();
      ByteBuffer byteBuffer0 = nullByteBufferPool0.acquire(236, true);
      CodedInputStream codedInputStream0 = CodedInputStream.newInstance(byteBuffer0);
      try { 
        HdfsServerProtos.ExportedBlockKeysProto.parseFrom(codedInputStream0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Message missing required fields: isBlockTokenEnabled, keyUpdateInterval, tokenLifeTime, currentKey
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.UninitializedMessageException", e);
      }
  }

  @Test(timeout = 4000)
  public void test067()  throws Throwable  {
      byte[] byteArray0 = new byte[6];
      try { 
        HdfsServerProtos.ExportedBlockKeysProto.parseFrom(byteArray0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Protocol message contained an invalid tag (zero).
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException", e);
      }
  }

  @Test(timeout = 4000)
  public void test068()  throws Throwable  {
      HdfsServerProtos.NNHAStatusHeartbeatProto.Builder hdfsServerProtos_NNHAStatusHeartbeatProto_Builder0 = HdfsServerProtos.NNHAStatusHeartbeatProto.newBuilder();
      Descriptors.Descriptor descriptors_Descriptor0 = Timestamp.getDescriptor();
      // Undeclared exception!
      try { 
        hdfsServerProtos_NNHAStatusHeartbeatProto_Builder0.setRepeatedField((Descriptors.FieldDescriptor) null, (-503), descriptors_Descriptor0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
      }
  }

  @Test(timeout = 4000)
  public void test069()  throws Throwable  {
      HdfsServerProtos.CheckpointSignatureProto.Builder hdfsServerProtos_CheckpointSignatureProto_Builder0 = HdfsServerProtos.CheckpointSignatureProto.newBuilder();
      HdfsServerProtos.CheckpointSignatureProto.Builder hdfsServerProtos_CheckpointSignatureProto_Builder1 = hdfsServerProtos_CheckpointSignatureProto_Builder0.clone();
      assertFalse(hdfsServerProtos_CheckpointSignatureProto_Builder1.hasBlockPoolId());
      assertFalse(hdfsServerProtos_CheckpointSignatureProto_Builder1.hasCurSegmentTxId());
      assertFalse(hdfsServerProtos_CheckpointSignatureProto_Builder1.hasStorageInfo());
      assertFalse(hdfsServerProtos_CheckpointSignatureProto_Builder1.hasMostRecentCheckpointTxId());
  }

  @Test(timeout = 4000)
  public void test070()  throws Throwable  {
      HdfsServerProtos.CheckpointSignatureProto hdfsServerProtos_CheckpointSignatureProto0 = HdfsServerProtos.CheckpointSignatureProto.getDefaultInstance();
      HdfsServerProtos.CheckpointSignatureProto.Builder hdfsServerProtos_CheckpointSignatureProto_Builder0 = HdfsServerProtos.CheckpointSignatureProto.newBuilder(hdfsServerProtos_CheckpointSignatureProto0);
      HdfsServerProtos.CheckpointSignatureProto.Builder hdfsServerProtos_CheckpointSignatureProto_Builder1 = hdfsServerProtos_CheckpointSignatureProto_Builder0.setUnknownFields((UnknownFieldSet) null);
      assertSame(hdfsServerProtos_CheckpointSignatureProto_Builder1, hdfsServerProtos_CheckpointSignatureProto_Builder0);
  }

  @Test(timeout = 4000)
  public void test071()  throws Throwable  {
      HdfsServerProtos.CheckpointSignatureProto.Builder hdfsServerProtos_CheckpointSignatureProto_Builder0 = HdfsServerProtos.CheckpointSignatureProto.newBuilder();
      hdfsServerProtos_CheckpointSignatureProto_Builder0.setCurSegmentTxId((byte)14);
      assertEquals(14L, hdfsServerProtos_CheckpointSignatureProto_Builder0.getCurSegmentTxId());
  }

  @Test(timeout = 4000)
  public void test072()  throws Throwable  {
      HdfsServerProtos.CheckpointSignatureProto.Builder hdfsServerProtos_CheckpointSignatureProto_Builder0 = HdfsServerProtos.CheckpointSignatureProto.newBuilder();
      Descriptors.OneofDescriptor descriptors_OneofDescriptor0 = mock(Descriptors.OneofDescriptor.class, new ViolatedAssumptionAnswer());
      doReturn((Descriptors.Descriptor) null).when(descriptors_OneofDescriptor0).getContainingType();
      // Undeclared exception!
      try { 
        hdfsServerProtos_CheckpointSignatureProto_Builder0.clearOneof(descriptors_OneofDescriptor0);
        fail("Expecting exception: IllegalArgumentException");
      
      } catch(IllegalArgumentException e) {
         //
         // OneofDescriptor does not match message type.
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3$FieldAccessorTable", e);
      }
  }

  @Test(timeout = 4000)
  public void test073()  throws Throwable  {
      ExtensionRegistryLite extensionRegistryLite0 = ExtensionRegistryLite.getEmptyRegistry();
      // Undeclared exception!
      try { 
        HdfsServerProtos.NamenodeCommandProto.parseDelimitedFrom((InputStream) null, extensionRegistryLite0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.AbstractParser", e);
      }
  }

  @Test(timeout = 4000)
  public void test074()  throws Throwable  {
      // Undeclared exception!
      try { 
        HdfsServerProtos.NamenodeCommandProto.parseDelimitedFrom((InputStream) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.AbstractParser", e);
      }
  }

  @Test(timeout = 4000)
  public void test075()  throws Throwable  {
      PipedInputStream pipedInputStream0 = new PipedInputStream();
      PushbackInputStream pushbackInputStream0 = new PushbackInputStream(pipedInputStream0);
      CodedInputStream codedInputStream0 = CodedInputStream.newInstance((InputStream) pushbackInputStream0, 2102);
      try { 
        HdfsServerProtos.NamenodeCommandProto.parseFrom(codedInputStream0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Pipe not connected
         //
         verifyException("java.io.PipedInputStream", e);
      }
  }

  @Test(timeout = 4000)
  public void test076()  throws Throwable  {
      HdfsServerProtos.NamenodeCommandProto hdfsServerProtos_NamenodeCommandProto0 = HdfsServerProtos.NamenodeCommandProto.getDefaultInstance();
      HdfsServerProtos.NamenodeCommandProto.Builder hdfsServerProtos_NamenodeCommandProto_Builder0 = HdfsServerProtos.NamenodeCommandProto.newBuilder(hdfsServerProtos_NamenodeCommandProto0);
      byte[] byteArray0 = new byte[3];
      ByteArrayInputStream byteArrayInputStream0 = new ByteArrayInputStream(byteArray0, 1, 3);
      PushbackInputStream pushbackInputStream0 = new PushbackInputStream(byteArrayInputStream0);
      CodedInputStream codedInputStream0 = CodedInputStream.newInstance((InputStream) pushbackInputStream0);
      try { 
        hdfsServerProtos_NamenodeCommandProto_Builder0.mergeFrom(codedInputStream0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Protocol message contained an invalid tag (zero).
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException", e);
      }
  }

  @Test(timeout = 4000)
  public void test077()  throws Throwable  {
      HdfsServerProtos.BlockKeyProto.Builder hdfsServerProtos_BlockKeyProto_Builder0 = HdfsServerProtos.BlockKeyProto.newBuilder();
      // Undeclared exception!
      try { 
        hdfsServerProtos_BlockKeyProto_Builder0.setRepeatedField((Descriptors.FieldDescriptor) null, 2, (Object) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
      }
  }

  @Test(timeout = 4000)
  public void test078()  throws Throwable  {
      HdfsServerProtos.NamenodeRegistrationProto.NamenodeRoleProto hdfsServerProtos_NamenodeRegistrationProto_NamenodeRoleProto0 = HdfsServerProtos.NamenodeRegistrationProto.NamenodeRoleProto.CHECKPOINT;
      Descriptors.EnumDescriptor descriptors_EnumDescriptor0 = hdfsServerProtos_NamenodeRegistrationProto_NamenodeRoleProto0.getDescriptorForType();
      assertEquals("hadoop.hdfs.NamenodeRegistrationProto.NamenodeRoleProto", descriptors_EnumDescriptor0.getFullName());
  }

  @Test(timeout = 4000)
  public void test079()  throws Throwable  {
      ExtensionRegistryLite extensionRegistryLite0 = ExtensionRegistryLite.getEmptyRegistry();
      // Undeclared exception!
      try { 
        HdfsServerProtos.NamenodeRegistrationProto.parseFrom((byte[]) null, extensionRegistryLite0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.AbstractParser", e);
      }
  }

  @Test(timeout = 4000)
  public void test080()  throws Throwable  {
      HdfsServerProtos.NamenodeRegistrationProto.Builder hdfsServerProtos_NamenodeRegistrationProto_Builder0 = HdfsServerProtos.NamenodeRegistrationProto.newBuilder();
      // Undeclared exception!
      try { 
        hdfsServerProtos_NamenodeRegistrationProto_Builder0.build();
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // Message missing required fields: rpcAddress, httpAddress, storageInfo
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.AbstractMessage$Builder", e);
      }
  }

  @Test(timeout = 4000)
  public void test081()  throws Throwable  {
      PipedInputStream pipedInputStream0 = new PipedInputStream();
      try { 
        HdfsServerProtos.NamenodeRegistrationProto.parseFrom((InputStream) pipedInputStream0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Pipe not connected
         //
         verifyException("java.io.PipedInputStream", e);
      }
  }

  @Test(timeout = 4000)
  public void test082()  throws Throwable  {
      ByteString byteString0 = ByteString.copyFromUtf8("vx@$4~");
      try { 
        HdfsServerProtos.NamenodeRegistrationProto.parseFrom(byteString0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Protocol message tag had invalid wire type.
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException", e);
      }
  }

  @Test(timeout = 4000)
  public void test083()  throws Throwable  {
      HdfsServerProtos.NamenodeRegistrationProto.Builder hdfsServerProtos_NamenodeRegistrationProto_Builder0 = HdfsServerProtos.NamenodeRegistrationProto.newBuilder();
      HdfsServerProtos.NamenodeRegistrationProto.Builder hdfsServerProtos_NamenodeRegistrationProto_Builder1 = hdfsServerProtos_NamenodeRegistrationProto_Builder0.clearRpcAddress();
      assertFalse(hdfsServerProtos_NamenodeRegistrationProto_Builder1.hasRole());
  }

  @Test(timeout = 4000)
  public void test084()  throws Throwable  {
      ExtensionRegistry extensionRegistry0 = ExtensionRegistry.newInstance();
      HdfsServerProtos.BlocksWithLocationsProto hdfsServerProtos_BlocksWithLocationsProto0 = HdfsServerProtos.BlocksWithLocationsProto.parseFrom((InputStream) null, (ExtensionRegistryLite) extensionRegistry0);
      Descriptors.OneofDescriptor descriptors_OneofDescriptor0 = mock(Descriptors.OneofDescriptor.class, new ViolatedAssumptionAnswer());
      doReturn((Descriptors.Descriptor) null).when(descriptors_OneofDescriptor0).getContainingType();
      // Undeclared exception!
      try { 
        hdfsServerProtos_BlocksWithLocationsProto0.getOneofFieldDescriptor(descriptors_OneofDescriptor0);
        fail("Expecting exception: IllegalArgumentException");
      
      } catch(IllegalArgumentException e) {
         //
         // OneofDescriptor does not match message type.
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3$FieldAccessorTable", e);
      }
  }

  @Test(timeout = 4000)
  public void test085()  throws Throwable  {
      byte[] byteArray0 = new byte[2];
      try { 
        HdfsServerProtos.VersionResponseProto.parseFrom(byteArray0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Protocol message contained an invalid tag (zero).
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException", e);
      }
  }

  @Test(timeout = 4000)
  public void test086()  throws Throwable  {
      HdfsServerProtos.VersionResponseProto hdfsServerProtos_VersionResponseProto0 = HdfsServerProtos.VersionResponseProto.getDefaultInstance();
      HdfsServerProtos.VersionResponseProto.Builder hdfsServerProtos_VersionResponseProto_Builder0 = HdfsServerProtos.VersionResponseProto.newBuilder(hdfsServerProtos_VersionResponseProto0);
      HdfsServerProtos.VersionResponseProto.Builder hdfsServerProtos_VersionResponseProto_Builder1 = hdfsServerProtos_VersionResponseProto_Builder0.clearInfo();
      assertSame(hdfsServerProtos_VersionResponseProto_Builder0, hdfsServerProtos_VersionResponseProto_Builder1);
  }

  @Test(timeout = 4000)
  public void test087()  throws Throwable  {
      // Undeclared exception!
      try { 
        HdfsServerProtos.VersionResponseProto.parseFrom((CodedInputStream) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionResponseProto$Builder", e);
      }
  }

  @Test(timeout = 4000)
  public void test088()  throws Throwable  {
      Descriptors.Descriptor descriptors_Descriptor0 = HdfsServerProtos.VersionResponseProto.getDescriptor();
      assertEquals("hadoop.hdfs.VersionResponseProto", descriptors_Descriptor0.getFullName());
  }

  @Test(timeout = 4000)
  public void test089()  throws Throwable  {
      ByteBuffer byteBuffer0 = ByteBuffer.allocateDirect(822);
      try { 
        HdfsServerProtos.VersionResponseProto.parseFrom(byteBuffer0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Protocol message contained an invalid tag (zero).
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException", e);
      }
  }

  @Test(timeout = 4000)
  public void test090()  throws Throwable  {
      PushbackInputStream pushbackInputStream0 = new PushbackInputStream((InputStream) null);
      ExtensionRegistry extensionRegistry0 = ExtensionRegistry.newInstance();
      try { 
        HdfsServerProtos.CheckpointCommandProto.parseDelimitedFrom((InputStream) pushbackInputStream0, (ExtensionRegistryLite) extensionRegistry0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Stream closed
         //
         verifyException("java.io.PushbackInputStream", e);
      }
  }

  @Test(timeout = 4000)
  public void test091()  throws Throwable  {
      byte[] byteArray0 = new byte[7];
      try { 
        HdfsServerProtos.CheckpointCommandProto.parseFrom(byteArray0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Protocol message contained an invalid tag (zero).
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException", e);
      }
  }

  @Test(timeout = 4000)
  public void test092()  throws Throwable  {
      // Undeclared exception!
      try { 
        HdfsServerProtos.CheckpointCommandProto.parseFrom((CodedInputStream) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointCommandProto$Builder", e);
      }
  }

  @Test(timeout = 4000)
  public void test093()  throws Throwable  {
      // Undeclared exception!
      try { 
        HdfsServerProtos.CheckpointCommandProto.newBuilder((HdfsServerProtos.CheckpointCommandProto) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$CheckpointCommandProto$Builder", e);
      }
  }

  @Test(timeout = 4000)
  public void test094()  throws Throwable  {
      byte[] byteArray0 = new byte[8];
      ExtensionRegistry extensionRegistry0 = ExtensionRegistry.newInstance();
      try { 
        HdfsServerProtos.CheckpointCommandProto.parseFrom(byteArray0, (ExtensionRegistryLite) extensionRegistry0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Protocol message contained an invalid tag (zero).
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException", e);
      }
  }

  @Test(timeout = 4000)
  public void test095()  throws Throwable  {
      Descriptors.Descriptor descriptors_Descriptor0 = HdfsServerProtos.NNHAStatusHeartbeatProto.getDescriptor();
      assertFalse(descriptors_Descriptor0.isExtendable());
  }

  @Test(timeout = 4000)
  public void test096()  throws Throwable  {
      TreeSet<ByteBuffer> treeSet0 = new TreeSet<ByteBuffer>();
      PriorityQueue<ByteBuffer> priorityQueue0 = new PriorityQueue<ByteBuffer>((SortedSet<? extends ByteBuffer>) treeSet0);
      CodedInputStream codedInputStream0 = CodedInputStream.newInstance((Iterable<ByteBuffer>) priorityQueue0);
      ExtensionRegistry extensionRegistry0 = ExtensionRegistry.getEmptyRegistry();
      try { 
        HdfsServerProtos.NNHAStatusHeartbeatProto.parseFrom(codedInputStream0, (ExtensionRegistryLite) extensionRegistry0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Message missing required fields: state, txid
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.UninitializedMessageException", e);
      }
  }

  @Test(timeout = 4000)
  public void test097()  throws Throwable  {
      byte[] byteArray0 = new byte[0];
      ByteString byteString0 = ByteString.copyFrom(byteArray0);
      ExtensionRegistryLite extensionRegistryLite0 = ExtensionRegistryLite.getEmptyRegistry();
      try { 
        HdfsServerProtos.NNHAStatusHeartbeatProto.parseFrom(byteString0, extensionRegistryLite0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Message missing required fields: state, txid
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.UninitializedMessageException", e);
      }
  }

  @Test(timeout = 4000)
  public void test098()  throws Throwable  {
      ByteBuffer byteBuffer0 = ByteBuffer.allocate(2049);
      try { 
        HdfsServerProtos.NNHAStatusHeartbeatProto.parseFrom(byteBuffer0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Protocol message contained an invalid tag (zero).
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException", e);
      }
  }

  @Test(timeout = 4000)
  public void test099()  throws Throwable  {
      ExtensionRegistry extensionRegistry0 = ExtensionRegistry.newInstance();
      // Undeclared exception!
      try { 
        HdfsServerProtos.NNHAStatusHeartbeatProto.parseDelimitedFrom((InputStream) null, (ExtensionRegistryLite) extensionRegistry0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.AbstractParser", e);
      }
  }

  @Test(timeout = 4000)
  public void test100()  throws Throwable  {
      HdfsServerProtos.NNHAStatusHeartbeatProto hdfsServerProtos_NNHAStatusHeartbeatProto0 = HdfsServerProtos.NNHAStatusHeartbeatProto.getDefaultInstance();
      assertFalse(hdfsServerProtos_NNHAStatusHeartbeatProto0.hasTxid());
  }

  @Test(timeout = 4000)
  public void test101()  throws Throwable  {
      byte[] byteArray0 = new byte[4];
      // Undeclared exception!
      try { 
        HdfsServerProtos.NNHAStatusHeartbeatProto.parseFrom(byteArray0, (ExtensionRegistryLite) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$NNHAStatusHeartbeatProto$Builder", e);
      }
  }

  @Test(timeout = 4000)
  public void test102()  throws Throwable  {
      // Undeclared exception!
      try { 
        HdfsServerProtos.NNHAStatusHeartbeatProto.parseDelimitedFrom((InputStream) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.AbstractParser", e);
      }
  }

  @Test(timeout = 4000)
  public void test103()  throws Throwable  {
      ByteBuffer byteBuffer0 = ByteBuffer.allocateDirect(330);
      ExtensionRegistryLite extensionRegistryLite0 = ExtensionRegistryLite.newInstance();
      try { 
        HdfsServerProtos.NNHAStatusHeartbeatProto.parseFrom(byteBuffer0, extensionRegistryLite0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Protocol message contained an invalid tag (zero).
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException", e);
      }
  }

  @Test(timeout = 4000)
  public void test104()  throws Throwable  {
      CodedInputStream codedInputStream0 = CodedInputStream.newInstance((InputStream) null);
      try { 
        HdfsServerProtos.NNHAStatusHeartbeatProto.parseFrom(codedInputStream0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Message missing required fields: state, txid
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.UninitializedMessageException", e);
      }
  }

  @Test(timeout = 4000)
  public void test105()  throws Throwable  {
      Descriptors.Descriptor descriptors_Descriptor0 = HdfsServerProtos.BlockWithLocationsProto.Builder.getDescriptor();
      assertFalse(descriptors_Descriptor0.isExtendable());
  }

  @Test(timeout = 4000)
  public void test106()  throws Throwable  {
      HdfsServerProtos.BlockWithLocationsProto.Builder hdfsServerProtos_BlockWithLocationsProto_Builder0 = HdfsServerProtos.BlockWithLocationsProto.newBuilder();
      // Undeclared exception!
      try { 
        hdfsServerProtos_BlockWithLocationsProto_Builder0.addAllStorageUuids((Iterable<String>) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.Internal", e);
      }
  }

  @Test(timeout = 4000)
  public void test107()  throws Throwable  {
      HdfsServerProtos.BlockWithLocationsProto.Builder hdfsServerProtos_BlockWithLocationsProto_Builder0 = HdfsServerProtos.BlockWithLocationsProto.newBuilder();
      HdfsServerProtos.BlockWithLocationsProto.Builder hdfsServerProtos_BlockWithLocationsProto_Builder1 = hdfsServerProtos_BlockWithLocationsProto_Builder0.clearCellSize();
      assertFalse(hdfsServerProtos_BlockWithLocationsProto_Builder1.hasBlock());
      assertEquals(0, hdfsServerProtos_BlockWithLocationsProto_Builder1.getCellSize());
  }

  @Test(timeout = 4000)
  public void test108()  throws Throwable  {
      HdfsServerProtos.BlockWithLocationsProto.Builder hdfsServerProtos_BlockWithLocationsProto_Builder0 = HdfsServerProtos.BlockWithLocationsProto.newBuilder();
      HdfsServerProtos.BlockWithLocationsProto.Builder hdfsServerProtos_BlockWithLocationsProto_Builder1 = hdfsServerProtos_BlockWithLocationsProto_Builder0.clearDataBlockNum();
      assertFalse(hdfsServerProtos_BlockWithLocationsProto_Builder1.hasBlock());
      assertEquals(0, hdfsServerProtos_BlockWithLocationsProto_Builder1.getDataBlockNum());
  }

  @Test(timeout = 4000)
  public void test109()  throws Throwable  {
      HdfsServerProtos.BlockWithLocationsProto.Builder hdfsServerProtos_BlockWithLocationsProto_Builder0 = HdfsServerProtos.BlockWithLocationsProto.newBuilder();
      hdfsServerProtos_BlockWithLocationsProto_Builder0.setDataBlockNum(3);
      assertEquals(3, hdfsServerProtos_BlockWithLocationsProto_Builder0.getDataBlockNum());
  }

  @Test(timeout = 4000)
  public void test110()  throws Throwable  {
      // Undeclared exception!
      try { 
        HdfsServerProtos.BlockWithLocationsProto.newBuilder((HdfsServerProtos.BlockWithLocationsProto) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto$Builder", e);
      }
  }

  @Test(timeout = 4000)
  public void test111()  throws Throwable  {
      Enumeration<InputStream> enumeration0 = (Enumeration<InputStream>) mock(Enumeration.class, new ViolatedAssumptionAnswer());
      doReturn(false).when(enumeration0).hasMoreElements();
      SequenceInputStream sequenceInputStream0 = new SequenceInputStream(enumeration0);
      // Undeclared exception!
      try { 
        HdfsServerProtos.BlockWithLocationsProto.parseFrom((InputStream) sequenceInputStream0, (ExtensionRegistryLite) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$BlockWithLocationsProto$Builder", e);
      }
  }

  @Test(timeout = 4000)
  public void test112()  throws Throwable  {
      HdfsServerProtos.BlockWithLocationsProto hdfsServerProtos_BlockWithLocationsProto0 = HdfsServerProtos.BlockWithLocationsProto.getDefaultInstance();
      // Undeclared exception!
      try { 
        hdfsServerProtos_BlockWithLocationsProto0.getStorageUuids(1);
        fail("Expecting exception: IndexOutOfBoundsException");
      
      } catch(IndexOutOfBoundsException e) {
         //
         // Index: 1, Size: 0
         //
         verifyException("java.util.ArrayList", e);
      }
  }

  @Test(timeout = 4000)
  public void test113()  throws Throwable  {
      HdfsProtos.DatanodeInfoProto hdfsProtos_DatanodeInfoProto0 = HdfsProtos.DatanodeInfoProto.getDefaultInstance();
      HdfsProtos.DatanodeIDProto hdfsProtos_DatanodeIDProto0 = hdfsProtos_DatanodeInfoProto0.getId();
      ByteString byteString0 = hdfsProtos_DatanodeIDProto0.getDatanodeUuidBytes();
      CodedInputStream codedInputStream0 = byteString0.newCodedInput();
      ExtensionRegistry extensionRegistry0 = ExtensionRegistry.getEmptyRegistry();
      try { 
        HdfsServerProtos.BlockWithLocationsProto.parseFrom(codedInputStream0, (ExtensionRegistryLite) extensionRegistry0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Message missing required fields: block
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.UninitializedMessageException", e);
      }
  }

  @Test(timeout = 4000)
  public void test114()  throws Throwable  {
      MappedByteBufferPool.Tagged mappedByteBufferPool_Tagged0 = new MappedByteBufferPool.Tagged();
      ByteBuffer byteBuffer0 = mappedByteBufferPool_Tagged0.newByteBuffer(9, false);
      ExtensionRegistry extensionRegistry0 = ExtensionRegistry.newInstance();
      try { 
        HdfsServerProtos.BlockWithLocationsProto.parseFrom(byteBuffer0, (ExtensionRegistryLite) extensionRegistry0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Message missing required fields: block
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.UninitializedMessageException", e);
      }
  }

  @Test(timeout = 4000)
  public void test115()  throws Throwable  {
      HdfsServerProtos.NamespaceInfoProto.Builder hdfsServerProtos_NamespaceInfoProto_Builder0 = HdfsServerProtos.NamespaceInfoProto.newBuilder();
      HdfsServerProtos.NamespaceInfoProto.Builder hdfsServerProtos_NamespaceInfoProto_Builder1 = hdfsServerProtos_NamespaceInfoProto_Builder0.setUnknownFields((UnknownFieldSet) null);
      assertEquals(HdfsServerProtos.NNHAStatusHeartbeatProto.State.ACTIVE, hdfsServerProtos_NamespaceInfoProto_Builder1.getState());
  }

  @Test(timeout = 4000)
  public void test116()  throws Throwable  {
      Descriptors.Descriptor descriptors_Descriptor0 = HdfsServerProtos.CheckpointCommandProto.Builder.getDescriptor();
      assertEquals("hadoop.hdfs.CheckpointCommandProto", descriptors_Descriptor0.getFullName());
  }

  @Test(timeout = 4000)
  public void test117()  throws Throwable  {
      HdfsServerProtos.RecoveringBlockProto.Builder hdfsServerProtos_RecoveringBlockProto_Builder0 = HdfsServerProtos.RecoveringBlockProto.newBuilder();
      GeneratedMessageV3.FieldAccessorTable generatedMessageV3_FieldAccessorTable0 = hdfsServerProtos_RecoveringBlockProto_Builder0.internalGetFieldAccessorTable();
      assertNotNull(generatedMessageV3_FieldAccessorTable0);
  }

  @Test(timeout = 4000)
  public void test118()  throws Throwable  {
      HdfsServerProtos.RecoveringBlockProto.Builder hdfsServerProtos_RecoveringBlockProto_Builder0 = HdfsServerProtos.RecoveringBlockProto.newBuilder();
      hdfsServerProtos_RecoveringBlockProto_Builder0.setNewGenStamp((-221L));
      assertTrue(hdfsServerProtos_RecoveringBlockProto_Builder0.hasNewGenStamp());
  }

  @Test(timeout = 4000)
  public void test119()  throws Throwable  {
      HdfsServerProtos.RecoveringBlockProto.Builder hdfsServerProtos_RecoveringBlockProto_Builder0 = HdfsServerProtos.RecoveringBlockProto.newBuilder();
      HdfsServerProtos.RecoveringBlockProto.Builder hdfsServerProtos_RecoveringBlockProto_Builder1 = hdfsServerProtos_RecoveringBlockProto_Builder0.clearNewGenStamp();
      assertEquals(0L, hdfsServerProtos_RecoveringBlockProto_Builder1.getNewGenStamp());
      assertFalse(hdfsServerProtos_RecoveringBlockProto_Builder1.hasEcPolicy());
  }

  @Test(timeout = 4000)
  public void test120()  throws Throwable  {
      Descriptors.Descriptor descriptors_Descriptor0 = HdfsServerProtos.RecoveringBlockProto.Builder.getDescriptor();
      assertEquals(7, descriptors_Descriptor0.getIndex());
  }

  @Test(timeout = 4000)
  public void test121()  throws Throwable  {
      HdfsServerProtos.NamespaceInfoProto.Builder hdfsServerProtos_NamespaceInfoProto_Builder0 = HdfsServerProtos.NamespaceInfoProto.newBuilder();
      HdfsServerProtos.NNHAStatusHeartbeatProto.State hdfsServerProtos_NNHAStatusHeartbeatProto_State0 = HdfsServerProtos.NNHAStatusHeartbeatProto.State.OBSERVER;
      hdfsServerProtos_NamespaceInfoProto_Builder0.setState(hdfsServerProtos_NNHAStatusHeartbeatProto_State0);
      assertEquals(HdfsServerProtos.NNHAStatusHeartbeatProto.State.OBSERVER, hdfsServerProtos_NamespaceInfoProto_Builder0.getState());
  }

  @Test(timeout = 4000)
  public void test122()  throws Throwable  {
      Internal.EnumLiteMap<HdfsServerProtos.NNHAStatusHeartbeatProto.State> internal_EnumLiteMap0 = HdfsServerProtos.NNHAStatusHeartbeatProto.State.internalGetValueMap();
      assertNotNull(internal_EnumLiteMap0);
  }

  @Test(timeout = 4000)
  public void test123()  throws Throwable  {
      HdfsServerProtos.StorageInfoProto.Builder hdfsServerProtos_StorageInfoProto_Builder0 = HdfsServerProtos.StorageInfoProto.newBuilder();
      // Undeclared exception!
      try { 
        hdfsServerProtos_StorageInfoProto_Builder0.mergeFrom((HdfsServerProtos.StorageInfoProto) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto$Builder", e);
      }
  }

  @Test(timeout = 4000)
  public void test124()  throws Throwable  {
      byte[] byteArray0 = new byte[4];
      CodedInputStream codedInputStream0 = CodedInputStream.newInstance(byteArray0);
      try { 
        HdfsServerProtos.StorageInfoProto.parseFrom(codedInputStream0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Protocol message contained an invalid tag (zero).
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException", e);
      }
  }

  @Test(timeout = 4000)
  public void test125()  throws Throwable  {
      byte[] byteArray0 = new byte[0];
      // Undeclared exception!
      try { 
        HdfsServerProtos.StorageInfoProto.parseFrom(byteArray0, (ExtensionRegistryLite) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$StorageInfoProto$Builder", e);
      }
  }

  @Test(timeout = 4000)
  public void test126()  throws Throwable  {
      HdfsServerProtos.NamenodeCommandProto.Builder hdfsServerProtos_NamenodeCommandProto_Builder0 = HdfsServerProtos.NamenodeCommandProto.newBuilder();
      hdfsServerProtos_NamenodeCommandProto_Builder0.setAction(1012);
      assertTrue(hdfsServerProtos_NamenodeCommandProto_Builder0.hasAction());
  }

  @Test(timeout = 4000)
  public void test127()  throws Throwable  {
      HdfsServerProtos.NamenodeCommandProto.Builder hdfsServerProtos_NamenodeCommandProto_Builder0 = HdfsServerProtos.NamenodeCommandProto.newBuilder();
      byte[] byteArray0 = new byte[8];
      ByteArrayInputStream byteArrayInputStream0 = new ByteArrayInputStream(byteArray0, (-1476), 0);
      ExtensionRegistryLite extensionRegistryLite0 = ExtensionRegistryLite.newInstance();
      DescriptorProtos.FileOptions descriptorProtos_FileOptions0 = DescriptorProtos.FileOptions.parseFrom((InputStream) byteArrayInputStream0, extensionRegistryLite0);
      // Undeclared exception!
      try { 
        hdfsServerProtos_NamenodeCommandProto_Builder0.mergeFrom((Message) descriptorProtos_FileOptions0);
        fail("Expecting exception: IllegalArgumentException");
      
      } catch(IllegalArgumentException e) {
         //
         // mergeFrom(Message) can only merge messages of the same type.
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.AbstractMessage$Builder", e);
      }
  }

  @Test(timeout = 4000)
  public void test128()  throws Throwable  {
      HdfsServerProtos.NamenodeCommandProto.Builder hdfsServerProtos_NamenodeCommandProto_Builder0 = HdfsServerProtos.NamenodeCommandProto.newBuilder();
      HdfsServerProtos.NamenodeCommandProto.Builder hdfsServerProtos_NamenodeCommandProto_Builder1 = hdfsServerProtos_NamenodeCommandProto_Builder0.setUnknownFields((UnknownFieldSet) null);
      assertEquals(HdfsServerProtos.NamenodeCommandProto.Type.NamenodeCommand, hdfsServerProtos_NamenodeCommandProto_Builder1.getType());
  }

  @Test(timeout = 4000)
  public void test129()  throws Throwable  {
      byte[] byteArray0 = new byte[7];
      ByteBuffer byteBuffer0 = ByteBuffer.wrap(byteArray0);
      ExtensionRegistryLite extensionRegistryLite0 = ExtensionRegistryLite.newInstance();
      try { 
        HdfsServerProtos.RecoveringBlockProto.parseFrom(byteBuffer0, extensionRegistryLite0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Protocol message contained an invalid tag (zero).
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException", e);
      }
  }

  @Test(timeout = 4000)
  public void test130()  throws Throwable  {
      FileDescriptor fileDescriptor0 = new FileDescriptor();
      MockFileInputStream mockFileInputStream0 = new MockFileInputStream(fileDescriptor0);
      ExtensionRegistry extensionRegistry0 = ExtensionRegistry.getEmptyRegistry();
      try { 
        HdfsServerProtos.RecoveringBlockProto.parseDelimitedFrom((InputStream) mockFileInputStream0, (ExtensionRegistryLite) extensionRegistry0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.evosuite.runtime.mock.java.io.NativeMockedIO", e);
      }
  }

  @Test(timeout = 4000)
  public void test131()  throws Throwable  {
      PipedInputStream pipedInputStream0 = new PipedInputStream();
      try { 
        HdfsServerProtos.RecoveringBlockProto.parseDelimitedFrom((InputStream) pipedInputStream0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Pipe not connected
         //
         verifyException("java.io.PipedInputStream", e);
      }
  }

  @Test(timeout = 4000)
  public void test132()  throws Throwable  {
      HdfsServerProtos.RecoveringBlockProto.Builder hdfsServerProtos_RecoveringBlockProto_Builder0 = HdfsServerProtos.RecoveringBlockProto.newBuilder();
      HdfsServerProtos.RecoveringBlockProto.Builder hdfsServerProtos_RecoveringBlockProto_Builder1 = hdfsServerProtos_RecoveringBlockProto_Builder0.clearBlockIndices();
      assertFalse(hdfsServerProtos_RecoveringBlockProto_Builder1.hasNewGenStamp());
  }

  @Test(timeout = 4000)
  public void test133()  throws Throwable  {
      try { 
        HdfsServerProtos.RecoveringBlockProto.parseFrom((InputStream) null);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Message missing required fields: newGenStamp, block
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.UninitializedMessageException", e);
      }
  }

  @Test(timeout = 4000)
  public void test134()  throws Throwable  {
      ByteBuffer byteBuffer0 = ByteBuffer.allocateDirect(255);
      CodedInputStream codedInputStream0 = CodedInputStream.newInstance(byteBuffer0);
      ExtensionRegistry extensionRegistry0 = ExtensionRegistry.getEmptyRegistry();
      try { 
        HdfsServerProtos.RecoveringBlockProto.parseFrom(codedInputStream0, (ExtensionRegistryLite) extensionRegistry0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Protocol message contained an invalid tag (zero).
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException", e);
      }
  }

  @Test(timeout = 4000)
  public void test135()  throws Throwable  {
      ByteString byteString0 = ByteString.empty();
      try { 
        HdfsServerProtos.BlockKeyProto.parseFrom(byteString0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Message missing required fields: keyId, expiryDate
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.UninitializedMessageException", e);
      }
  }

  @Test(timeout = 4000)
  public void test136()  throws Throwable  {
      byte[] byteArray0 = new byte[4];
      try { 
        HdfsServerProtos.BlockKeyProto.parseFrom(byteArray0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Protocol message contained an invalid tag (zero).
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException", e);
      }
  }

  @Test(timeout = 4000)
  public void test137()  throws Throwable  {
      HdfsServerProtos.BlocksWithLocationsProto hdfsServerProtos_BlocksWithLocationsProto0 = HdfsServerProtos.BlocksWithLocationsProto.getDefaultInstance();
      HdfsServerProtos.BlocksWithLocationsProto.Builder hdfsServerProtos_BlocksWithLocationsProto_Builder0 = hdfsServerProtos_BlocksWithLocationsProto0.newBuilderForType();
      hdfsServerProtos_BlocksWithLocationsProto_Builder0.addBlocksBuilder();
      hdfsServerProtos_BlocksWithLocationsProto_Builder0.clear();
      assertEquals(0, hdfsServerProtos_BlocksWithLocationsProto_Builder0.getBlocksCount());
  }

  @Test(timeout = 4000)
  public void test138()  throws Throwable  {
      HdfsServerProtos.VersionRequestProto.Builder hdfsServerProtos_VersionRequestProto_Builder0 = HdfsServerProtos.VersionRequestProto.newBuilder();
      HdfsServerProtos.VersionRequestProto.Builder hdfsServerProtos_VersionRequestProto_Builder1 = hdfsServerProtos_VersionRequestProto_Builder0.clear();
      assertTrue(hdfsServerProtos_VersionRequestProto_Builder1.isInitialized());
  }

  @Test(timeout = 4000)
  public void test139()  throws Throwable  {
      Descriptors.Descriptor descriptors_Descriptor0 = HdfsServerProtos.VersionRequestProto.Builder.getDescriptor();
      HdfsServerProtos.NamenodeCommandProto.Builder hdfsServerProtos_NamenodeCommandProto_Builder0 = HdfsServerProtos.NamenodeCommandProto.newBuilder();
      // Undeclared exception!
      try { 
        hdfsServerProtos_NamenodeCommandProto_Builder0.setField((Descriptors.FieldDescriptor) null, descriptors_Descriptor0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
      }
  }

  @Test(timeout = 4000)
  public void test140()  throws Throwable  {
      HdfsServerProtos.VersionRequestProto.Builder hdfsServerProtos_VersionRequestProto_Builder0 = HdfsServerProtos.VersionRequestProto.newBuilder();
      ByteString byteString0 = ByteString.EMPTY;
      UnknownFieldSet unknownFieldSet0 = UnknownFieldSet.parseFrom(byteString0);
      HdfsServerProtos.VersionRequestProto.Builder hdfsServerProtos_VersionRequestProto_Builder1 = hdfsServerProtos_VersionRequestProto_Builder0.setUnknownFields(unknownFieldSet0);
      assertTrue(hdfsServerProtos_VersionRequestProto_Builder1.isInitialized());
  }

  @Test(timeout = 4000)
  public void test141()  throws Throwable  {
      HdfsServerProtos.RemoteEditLogProto.Builder hdfsServerProtos_RemoteEditLogProto_Builder0 = HdfsServerProtos.RemoteEditLogProto.newBuilder();
      Descriptors.OneofDescriptor descriptors_OneofDescriptor0 = mock(Descriptors.OneofDescriptor.class, new ViolatedAssumptionAnswer());
      doReturn((Descriptors.Descriptor) null).when(descriptors_OneofDescriptor0).getContainingType();
      // Undeclared exception!
      try { 
        hdfsServerProtos_RemoteEditLogProto_Builder0.clearOneof(descriptors_OneofDescriptor0);
        fail("Expecting exception: IllegalArgumentException");
      
      } catch(IllegalArgumentException e) {
         //
         // OneofDescriptor does not match message type.
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3$FieldAccessorTable", e);
      }
  }

  @Test(timeout = 4000)
  public void test142()  throws Throwable  {
      Descriptors.Descriptor descriptors_Descriptor0 = HdfsServerProtos.RemoteEditLogProto.Builder.getDescriptor();
      HdfsServerProtos.VersionRequestProto.Builder hdfsServerProtos_VersionRequestProto_Builder0 = HdfsServerProtos.VersionRequestProto.newBuilder();
      Descriptors.OneofDescriptor descriptors_OneofDescriptor0 = mock(Descriptors.OneofDescriptor.class, new ViolatedAssumptionAnswer());
      doReturn(descriptors_Descriptor0).when(descriptors_OneofDescriptor0).getContainingType();
      // Undeclared exception!
      try { 
        hdfsServerProtos_VersionRequestProto_Builder0.clearOneof(descriptors_OneofDescriptor0);
        fail("Expecting exception: IllegalArgumentException");
      
      } catch(IllegalArgumentException e) {
         //
         // OneofDescriptor does not match message type.
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3$FieldAccessorTable", e);
      }
  }

  @Test(timeout = 4000)
  public void test143()  throws Throwable  {
      HdfsServerProtos.ExportedBlockKeysProto.Builder hdfsServerProtos_ExportedBlockKeysProto_Builder0 = HdfsServerProtos.ExportedBlockKeysProto.newBuilder();
      Descriptors.OneofDescriptor descriptors_OneofDescriptor0 = mock(Descriptors.OneofDescriptor.class, new ViolatedAssumptionAnswer());
      doReturn((Descriptors.Descriptor) null).when(descriptors_OneofDescriptor0).getContainingType();
      // Undeclared exception!
      try { 
        hdfsServerProtos_ExportedBlockKeysProto_Builder0.hasOneof(descriptors_OneofDescriptor0);
        fail("Expecting exception: IllegalArgumentException");
      
      } catch(IllegalArgumentException e) {
         //
         // OneofDescriptor does not match message type.
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.GeneratedMessageV3$FieldAccessorTable", e);
      }
  }

  @Test(timeout = 4000)
  public void test144()  throws Throwable  {
      Descriptors.Descriptor descriptors_Descriptor0 = HdfsServerProtos.ExportedBlockKeysProto.Builder.getDescriptor();
      assertFalse(descriptors_Descriptor0.isExtendable());
  }

  @Test(timeout = 4000)
  public void test145()  throws Throwable  {
      HdfsServerProtos.RemoteEditLogManifestProto hdfsServerProtos_RemoteEditLogManifestProto0 = HdfsServerProtos.RemoteEditLogManifestProto.parseFrom((InputStream) null);
      assertEquals(0L, hdfsServerProtos_RemoteEditLogManifestProto0.getCommittedTxnId());
  }

  @Test(timeout = 4000)
  public void test146()  throws Throwable  {
      PipedInputStream pipedInputStream0 = new PipedInputStream();
      try { 
        HdfsServerProtos.RemoteEditLogManifestProto.parseFrom((InputStream) pipedInputStream0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Pipe not connected
         //
         verifyException("java.io.PipedInputStream", e);
      }
  }

  @Test(timeout = 4000)
  public void test147()  throws Throwable  {
      byte[] byteArray0 = new byte[6];
      CodedInputStream codedInputStream0 = CodedInputStream.newInstance(byteArray0, 1386, 28);
      ExtensionRegistry extensionRegistry0 = ExtensionRegistry.newInstance();
      // Undeclared exception!
      try { 
        HdfsServerProtos.RemoteEditLogManifestProto.parseFrom(codedInputStream0, (ExtensionRegistryLite) extensionRegistry0);
        fail("Expecting exception: ArrayIndexOutOfBoundsException");
      
      } catch(ArrayIndexOutOfBoundsException e) {
         //
         // no message in exception (getMessage() returned null)
         //
      }
  }

  @Test(timeout = 4000)
  public void test148()  throws Throwable  {
      byte[] byteArray0 = new byte[8];
      ExtensionRegistry extensionRegistry0 = ExtensionRegistry.newInstance();
      try { 
        HdfsServerProtos.RemoteEditLogManifestProto.parseFrom(byteArray0, (ExtensionRegistryLite) extensionRegistry0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Protocol message contained an invalid tag (zero).
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException", e);
      }
  }

  @Test(timeout = 4000)
  public void test149()  throws Throwable  {
      HdfsServerProtos.RemoteEditLogManifestProto.Builder hdfsServerProtos_RemoteEditLogManifestProto_Builder0 = HdfsServerProtos.RemoteEditLogManifestProto.newBuilder();
      GeneratedMessageV3.FieldAccessorTable generatedMessageV3_FieldAccessorTable0 = hdfsServerProtos_RemoteEditLogManifestProto_Builder0.internalGetFieldAccessorTable();
      assertNotNull(generatedMessageV3_FieldAccessorTable0);
  }

  @Test(timeout = 4000)
  public void test150()  throws Throwable  {
      HdfsServerProtos.RemoteEditLogManifestProto hdfsServerProtos_RemoteEditLogManifestProto0 = HdfsServerProtos.RemoteEditLogManifestProto.getDefaultInstance();
      assertFalse(hdfsServerProtos_RemoteEditLogManifestProto0.hasCommittedTxnId());
  }

  @Test(timeout = 4000)
  public void test151()  throws Throwable  {
      HdfsServerProtos.StorageInfoProto.Builder hdfsServerProtos_StorageInfoProto_Builder0 = HdfsServerProtos.StorageInfoProto.newBuilder();
      HdfsServerProtos.StorageInfoProto.Builder hdfsServerProtos_StorageInfoProto_Builder1 = hdfsServerProtos_StorageInfoProto_Builder0.clearNamespceID();
      assertEquals(0, hdfsServerProtos_StorageInfoProto_Builder1.getNamespceID());
      assertFalse(hdfsServerProtos_StorageInfoProto_Builder1.hasLayoutVersion());
  }

  @Test(timeout = 4000)
  public void test152()  throws Throwable  {
      HdfsServerProtos.StorageInfoProto.Builder hdfsServerProtos_StorageInfoProto_Builder0 = HdfsServerProtos.StorageInfoProto.newBuilder();
      Map<Descriptors.FieldDescriptor, Object> map0 = hdfsServerProtos_StorageInfoProto_Builder0.getAllFields();
      assertEquals(0, map0.size());
  }

  @Test(timeout = 4000)
  public void test153()  throws Throwable  {
      HdfsServerProtos.StorageInfoProto.Builder hdfsServerProtos_StorageInfoProto_Builder0 = HdfsServerProtos.StorageInfoProto.newBuilder();
      hdfsServerProtos_StorageInfoProto_Builder0.setCTime(1L);
      assertEquals(1L, hdfsServerProtos_StorageInfoProto_Builder0.getCTime());
  }

  @Test(timeout = 4000)
  public void test154()  throws Throwable  {
      HdfsServerProtos.StorageInfoProto.Builder hdfsServerProtos_StorageInfoProto_Builder0 = HdfsServerProtos.StorageInfoProto.newBuilder();
      hdfsServerProtos_StorageInfoProto_Builder0.setLayoutVersion(4374);
      assertTrue(hdfsServerProtos_StorageInfoProto_Builder0.hasLayoutVersion());
  }

  @Test(timeout = 4000)
  public void test155()  throws Throwable  {
      Descriptors.Descriptor descriptors_Descriptor0 = HdfsServerProtos.StorageInfoProto.Builder.getDescriptor();
      assertEquals(13, descriptors_Descriptor0.getIndex());
  }

  @Test(timeout = 4000)
  public void test156()  throws Throwable  {
      HdfsServerProtos.StorageInfoProto.Builder hdfsServerProtos_StorageInfoProto_Builder0 = HdfsServerProtos.StorageInfoProto.newBuilder();
      HdfsServerProtos.StorageInfoProto.Builder hdfsServerProtos_StorageInfoProto_Builder1 = hdfsServerProtos_StorageInfoProto_Builder0.clear();
      assertEquals(0, hdfsServerProtos_StorageInfoProto_Builder1.getLayoutVersion());
      assertEquals(0L, hdfsServerProtos_StorageInfoProto_Builder1.getCTime());
      assertFalse(hdfsServerProtos_StorageInfoProto_Builder1.hasLayoutVersion());
      assertEquals(0, hdfsServerProtos_StorageInfoProto_Builder1.getNamespceID());
  }

  @Test(timeout = 4000)
  public void test157()  throws Throwable  {
      byte[] byteArray0 = new byte[6];
      ByteArrayInputStream byteArrayInputStream0 = new ByteArrayInputStream(byteArray0);
      ExtensionRegistry extensionRegistry0 = ExtensionRegistry.getEmptyRegistry();
      try { 
        HdfsServerProtos.NamespaceInfoProto.parseDelimitedFrom((InputStream) byteArrayInputStream0, (ExtensionRegistryLite) extensionRegistry0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Message missing required fields: buildVersion, unused, blockPoolID, storageInfo, softwareVersion
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.UninitializedMessageException", e);
      }
  }

  @Test(timeout = 4000)
  public void test158()  throws Throwable  {
      try { 
        HdfsServerProtos.NamespaceInfoProto.parseFrom((InputStream) null);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Message missing required fields: buildVersion, unused, blockPoolID, storageInfo, softwareVersion
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.UninitializedMessageException", e);
      }
  }

  @Test(timeout = 4000)
  public void test159()  throws Throwable  {
      HdfsServerProtos.NamespaceInfoProto hdfsServerProtos_NamespaceInfoProto0 = HdfsServerProtos.NamespaceInfoProto.getDefaultInstance();
      assertEquals(6, HdfsServerProtos.NamespaceInfoProto.CAPABILITIES_FIELD_NUMBER);
  }

  @Test(timeout = 4000)
  public void test160()  throws Throwable  {
      Descriptors.Descriptor descriptors_Descriptor0 = HdfsServerProtos.VersionRequestProto.getDescriptor();
      HdfsServerProtos.RemoteEditLogManifestProto.Builder hdfsServerProtos_RemoteEditLogManifestProto_Builder0 = HdfsServerProtos.RemoteEditLogManifestProto.newBuilder();
      DynamicMessage.Builder dynamicMessage_Builder0 = DynamicMessage.newBuilder(descriptors_Descriptor0);
      DynamicMessage dynamicMessage0 = dynamicMessage_Builder0.buildPartial();
      UnknownFieldSet unknownFieldSet0 = dynamicMessage0.getUnknownFields();
      HdfsServerProtos.RemoteEditLogManifestProto.Builder hdfsServerProtos_RemoteEditLogManifestProto_Builder1 = hdfsServerProtos_RemoteEditLogManifestProto_Builder0.mergeUnknownFields(unknownFieldSet0);
      assertEquals(0, hdfsServerProtos_RemoteEditLogManifestProto_Builder1.getLogsCount());
  }

  @Test(timeout = 4000)
  public void test161()  throws Throwable  {
      HdfsServerProtos.NamenodeRegistrationProto.Builder hdfsServerProtos_NamenodeRegistrationProto_Builder0 = HdfsServerProtos.NamenodeRegistrationProto.newBuilder();
      ByteString byteString0 = ByteString.empty();
      HdfsServerProtos.VersionRequestProto hdfsServerProtos_VersionRequestProto0 = HdfsServerProtos.VersionRequestProto.parseFrom(byteString0);
      // Undeclared exception!
      try { 
        hdfsServerProtos_NamenodeRegistrationProto_Builder0.mergeFrom((Message) hdfsServerProtos_VersionRequestProto0);
        fail("Expecting exception: IllegalArgumentException");
      
      } catch(IllegalArgumentException e) {
         //
         // mergeFrom(Message) can only merge messages of the same type.
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.AbstractMessage$Builder", e);
      }
  }

  @Test(timeout = 4000)
  public void test162()  throws Throwable  {
      HdfsServerProtos.VersionRequestProto.Builder hdfsServerProtos_VersionRequestProto_Builder0 = HdfsServerProtos.VersionRequestProto.newBuilder();
      HdfsServerProtos.VersionRequestProto.Builder hdfsServerProtos_VersionRequestProto_Builder1 = hdfsServerProtos_VersionRequestProto_Builder0.clone();
      assertNotSame(hdfsServerProtos_VersionRequestProto_Builder1, hdfsServerProtos_VersionRequestProto_Builder0);
  }

  @Test(timeout = 4000)
  public void test163()  throws Throwable  {
      // Undeclared exception!
      try { 
        HdfsServerProtos.VersionRequestProto.parseFrom((byte[]) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.AbstractParser", e);
      }
  }

  @Test(timeout = 4000)
  public void test164()  throws Throwable  {
      byte[] byteArray0 = new byte[5];
      ExtensionRegistryLite extensionRegistryLite0 = ExtensionRegistryLite.getEmptyRegistry();
      try { 
        HdfsServerProtos.VersionRequestProto.parseFrom(byteArray0, extensionRegistryLite0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Protocol message contained an invalid tag (zero).
         //
         verifyException("org.apache.hadoop.thirdparty.protobuf.InvalidProtocolBufferException", e);
      }
  }

  @Test(timeout = 4000)
  public void test165()  throws Throwable  {
      // Undeclared exception!
      try { 
        HdfsServerProtos.VersionRequestProto.parseFrom((CodedInputStream) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.apache.hadoop.hdfs.protocol.proto.HdfsServerProtos$VersionRequestProto$Builder", e);
      }
  }

  @Test(timeout = 4000)
  public void test166()  throws Throwable  {
      PipedInputStream pipedInputStream0 = new PipedInputStream();
      try { 
        HdfsServerProtos.VersionRequestProto.parseFrom((InputStream) pipedInputStream0);
        fail("Expecting exception: IOException");
      
      } catch(IOException e) {
         //
         // Pipe not connected
         //
         verifyException("java.io.PipedInputStream", e);
      }
  }
}
